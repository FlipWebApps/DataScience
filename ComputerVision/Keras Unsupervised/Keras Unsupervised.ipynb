{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Unsupervised\n",
    "Unsupervised clustering of MNIST using KMenas, Autoencoder to produce embedding + KMeans and DEC (Unsupervised Deep Embedding for Clustering Analysis).\n",
    "\n",
    "DEC: https://arxiv.org/abs/1511.06335"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras.backend as K\n",
    "\n",
    "from time import time\n",
    "\n",
    "from keras import callbacks\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Dense, Input\n",
    "from keras.initializers import VarianceScaling\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.misc import imread\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score, normalized_mutual_info_score, adjusted_rand_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the mnist data and show a few samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To stop potential randomness\n",
    "seed = 128\n",
    "rng = np.random.RandomState(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "(10000, 28, 28)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAi8AAAI4CAYAAACiBwlnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzs3Xu8VmWZ//HvBeIZEEKJTMUDHtAUE83TT208E4rmaJLgIUecTMNSkzHHKFPJzPJcqAQmo9mAgpWjpCjmgeEwlAgU6IAiW8ADgoggcP3+2Itpy702e+3nuO7n+bxfr/3ae3/3Otxrc/FwsZ57rWXuLgAAgFi0qfYAAAAAWoPmBQAARIXmBQAARIXmBQAARIXmBQAARIXmBQAARIXmpYzMbKiZPVjtcQCVQL2jnlDv1UXzUiQz+7qZTTWzD82swcyeMLMjqzSW+Wa2KhnLh2b2VDXGgdqVs3rvbmYTzewjM5tjZsdVYxyoXXmq9yZjOtrM3Mx+XM1xVBvNSxHM7LuSfiHpRkldJe0s6W5J/ao4rFPcfdvk44QqjgM1Jof1/pCk/5H0GUnfl/SfZrZ9lcaCGpPDepeZtZN0m6TJ1RpDXtC8FMjMOkr6kaRvuftYd1/p7p+4++PuflUz6/zOzN42sw/MbJKZ7dvkZ33MbJaZrTCzt8zsyiTvYma/N7NlZvaemT1vZvy5oaLyVu9mtqekL0r6gbuvcvcxkl6RdEY5jh/1JW/13sQVkp6SNKeEhxsl/hEs3GGStpT0aCvWeUJSD0k7SJouaXSTn90v6WJ3by9pP0nPJPkVkhZK2l6N3f81kjb1TIfRZrbUzJ4yswNaMTZgU/JW7/tKet3dVzTJ/pLkQLHyVu8ys10kfUONTVXdo3kp3GckvePua7Ou4O4j3H2Fu6+WNFTSAUmHL0mfSOppZh3c/X13n94k7yZpl6Tzf96bfyDVOZK6S9pF0kRJT5rZdq0+MiCUt3rfVtIHG2UfSGrfimMCmpO3epek2yX9u7t/WNAR1Rial8K9K6mLmW2WZWEza2tmw8zsNTNbLml+8qMuyeczJPWRtMDMnjOzw5L8p5LmSXrKzF43syHN7cPdX0hOoX/k7jdJWibp/7X+0IBA3ur9Q0kdNso6SFqRsizQWrmqdzM7RVJ7d/9tgcdTc2heCveSpI8lnZZx+a+rcaLXcZI6qvEMiSSZJLn7FHfvp8ZTjo9JeiTJV7j7Fe6+m6RTJH3XzI7NuE/fsH2gSHmr91cl7WZmTc+0HJDkQLHyVu/HSuqdzKl5W9LXJF1uZuMKObhaQPNSIHf/QNJ1ku4ys9PMbGsza2dmJ5vZzSmrtJe0Wo0d/dZqnMEuSTKzzc3sHDPr6O6fSFouaV3ys75mtoeZWZN83cYbN7OdzeyIZFtbmtlVauz6XyjtkaMe5a3e3f3vkmZI+kFS76dL2l/SmFIeN+pT3upd0r9L2lNSr+RjvKR7JV1QokOODs1LEdz9VknflXStpKWS3pR0qRo76409IGmBpLckzZL08kY/HyhpfnLK8V8lDUjyHpL+pMbT5C9Jutvdn03ZfntJ90h6P9nHSZJOdvd3Czw84FNyVu+SdLak3mqs+WGS/tndlxZybMDG8lTvyRmatzd8SFolaaW7v1fUQUbMmp8bBAAAkD+ceQEAAFGheQEAAFGheQEAAFGheQEAAFHJdAOe5pjZSWp8SFRbSfe5+7AWlmd2MIr1jrtX7eF7ral56h0lEE29J8tT8yiKu2e6N1nBZ17MrK2kuySdLKmnpP5m1rPQ7QEZLajWjql5VAH1DqQo5m2jQyTNc/fX3X2NpIdVxUeFAxVAzaOeUO/IrWKalx3VeNOeDRYm2aeY2SAzm2pmU4vYF5AHLdY89Y4awms8cquYOS9p70sF73e6+3BJwyXeD0X0Wqx56h01hNd45FYxZ14WStqpyfefl7SouOEAuUbNo55Q78itYpqXKZJ6mNmuZra5Gp8zMr40wwJyiZpHPaHekVsFv23k7mvN7FJJT6rxMroR7s7j6FGzqHnUE+odeVbRBzPyfihKYJq79672ILKg3lEC0dS7RM2jeGW/zwsAAEA10LwAAICo0LwAAICo0LwAAICo0LwAAICo0LwAAICo0LwAAICo0LwAAICo0LwAAICo0LwAAICo0LwAAICo0LwAAICoFPxUaQAoh4MOOijILr300iA799xzg+yBBx4IsjvuuCN1P9OnTy9gdADygDMvAAAgKjQvAAAgKjQvAAAgKjQvAAAgKubuha9sNl/SCknrJK11994tLF/4ziLWtm3bIOvYsWNR20ybwLj11lsH2V577RVk3/rWt1K3ecsttwRZ//79g+zjjz8OsmHDhgXZD3/4w9T9FGlaS3VWTq2p+Xqt96x69eqVmj/zzDNB1qFDh4L388EHH6Tmn/nMZwreZgVFU+/J8tR8jh177LFBNnr06CA7+uijg+xvf/tbWca0MXe3LMuV4mqjL7v7OyXYDhALah71hHpH7vC2EQAAiEqxzYtLesrMppnZoLQFzGyQmU01s6lF7gvIg03WPPWOGsNrPHKp2LeNjnD3RWa2g6QJZjbH3Sc1XcDdh0saLvF+KGrCJmueekeN4TUeuVRU8+Lui5LPS8zsUUmHSJq06bXybeedd07NN9988yA7/PDDg+zII48Msu222y7IzjjjjAJG13oLFy4Msttvvz112dNPPz3IVqxYEWR/+ctfguy5554rYHTxqcWar4RDDjkkyMaMGZO6bNpk9rQLC9Jqc82aNUHW3MTcQw89NMjS7rqbts16kYd6P+qoo4Is7c/00UcfrcRwonbwwQcH2ZQpU6owkuIV/LaRmW1jZu03fC3pBEkzSzUwIG+oedQT6h15VsyZl66SHjWzDdv5D3f/r5KMCsgnah71hHpHbhXcvLj765IOKOFYgFyj5lFPqHfkGZdKAwCAqJTiJnXRSrvDZ9rdPaXi74hbCevXrw+ya6+9Nsg+/PDD1PXT7rTY0NAQZO+//36QVerui8iXtLs6f/GLXwyyBx98MMi6detW1L7nzp0bZDfffHOQPfzww6nrv/DCC0GW9vflpptuKmB0KJVjjjkmyHr06BFkTNj9hzZt0s9L7LrrrkG2yy67BFnyVmGuceYFAABEheYFAABEheYFAABEheYFAABEpa4n7L7xxhtB9u6776YuW4kJu5MnT07Nly1bFmRf/vKXgyztTqC/+c1vih8Y0Ixf/epXQda/f/+K7DttYvC2224bZM3d/TltIuj+++9f9LhQWueee26QvfTSS1UYSTyamwx/0UUXBVnaZPo5c+aUfEylxpkXAAAQFZoXAAAQFZoXAAAQFZoXAAAQFZoXAAAQlbq+2ui9994Lsquuuip12b59+wbZ//zP/wTZ7bffnmnfM2bMCLLjjz8+ddmVK1cG2b777htkgwcPzrRvoBAHHXRQkH3lK18Jsqy3Fm/uKqDHH388yG655ZYgW7RoUZCl/Z1Me5yFJP3TP/1TkMVwW/R609yt7tG8++67L/OyaY/ZiAFVAQAAokLzAgAAokLzAgAAotJi82JmI8xsiZnNbJJ1NrMJZjY3+dypvMMEKoeaRz2h3hEjc/dNL2B2lKQPJT3g7vsl2c2S3nP3YWY2RFInd7+6xZ2ZbXpnOdahQ4cgW7FiRZCl3S79wgsvDLIBAwYE2UMPPVTg6OrKNHfvXc4dlKrmY673Xr16BdkzzzwTZGl/L9I88cQTQdbcYwSOPvroIEu7bX/apMSlS5dmGo8krVu3Lsg++uijTOOZPn165v0UKZp6T9YrqubT/pzTHgUwduzYIBs4cGAxu64pL774Ymp+6KGHBtnhhx8eZC+//HLJx5SVu2eaNd/imRd3nyRp48ty+kkalXw9StJprRodkGPUPOoJ9Y4YFTrnpau7N0hS8nmH0g0JyCVqHvWEekeulf0+L2Y2SNKgcu8HyAPqHfWGmkc1FHrmZbGZdZOk5POS5hZ09+Hu3rvc79sCZZap5ql31Ahe45FrhZ55GS/pPEnDks/jSjainFq+fHmm5T744INMy1100UVB9tvf/jZ12fXr12faJsqqJmt+zz33TM3T7jTdsWPHIHvnnXeCrKGhIchGjRoVZB9++GHqvv/whz9kysphq622CrIrrrgiyM4555xKDKeaqlLvffr0CbK0PxP8Q9euXYNs1113zbz+W2+9VcrhVEyWS6UfkvSSpL3MbKGZXajGgj7ezOZKOj75HqgJ1DzqCfWOGLV45sXd069nlI4t8ViAXKDmUU+od8SIO+wCAICo0LwAAIColP1S6XozdOjQIDvooIOCLO2unccdd1zqNp966qmixwVsscUWQXbLLbekLps2cTLtjtLnnntukE2dOjXIYp50ufPOO1d7CHVjr732yrTcq6++WuaRxCPt73DaJF5J+vvf/x5kaX+vY8CZFwAAEBWaFwAAEBWaFwAAEBWaFwAAEBUm7JbYypUrgyztbrrTp08PsnvvvTd1mxMnTgyytEmRd911V5C5F/WEetSQAw88MMjSJuY2p1+/fkH23HPPFTUmoBBTpkyp9hBKqkOHDkF20kknBdmAAQOC7IQTTsi8n+uvvz7Ili1blnn9POHMCwAAiArNCwAAiArNCwAAiArNCwAAiAoTdivgtddeC7Lzzz8/yH7961+nrj9w4MBM2TbbbBNkDzzwQJA1NDSk7ge17dZbbw0yM0tdNm0ibq1Nzm3TJvy/2/r166swErRW586dS77NAw44IMjS/n6k3Qn985//fJBtvvnmQXbOOeek7jutFletWhVkkydPDrLVq1cH2Wabpf/TPm3atNQ8Rpx5AQAAUaF5AQAAUaF5AQAAUaF5AQAAUWmxeTGzEWa2xMxmNsmGmtlbZjYj+ch+m04g56h51BPqHTHKcrXRSEl3Str4spWfu/stJR9RnXj00UeDbO7cuanLpl0lcuyxxwbZjTfeGGS77LJLkN1www2p+3nrrbdS8zo0UpHXfN++fYOsV69eQdbc4yPGjx9f8jHlTdqVRWm/jxkzZlRiONU0Ujmp97QrbNL+TH75y18G2TXXXFPUvvfff/8gS7vaaO3atUH20UcfBdmsWbOCbMSIEan7TnvcS9rVfYsXLw6yhQsXBtlWW22Vup85c+ak5jFq8cyLu0+S9F4FxgLkAjWPekK9I0bFzHm51Mz+mpxy7NTcQmY2yMymmlnYWgJxabHmqXfUEF7jkVuFNi/3SNpdUi9JDZJ+1tyC7j7c3Xu7e+8C9wXkQaaap95RI3iNR64V1Ly4+2J3X+fu6yXdK+mQ0g4LyBdqHvWEekfeFfR4ADPr5u4b7jF/uqSZm1oe2cycmf5rPOuss4LslFNOCbK0xwtcfPHFQdajR4/U/Rx//PEtDbFuxVbzaRP20m5XvmTJktT1f/vb35Z8TJWwxRZbBNnQoUMzr//MM88E2b/9278VM6QoVaveL7nkkiBbsGBBkB1++OEl3/cbb7wRZI899liQzZ49O8hefvnlko8nzaBBg4Js++23D7LXX3+9EsOpqhabFzN7SNIxkrqY2UJJP5B0jJn1kuSS5ksK/4UEIkXNo55Q74hRi82Lu/dPie8vw1iAXKDmUU+od8SIO+wCAICo0LwAAICoFDRhF5W1bNmyIPvNb34TZPfdd1+QbbZZ+Ed81FFHpe7nmGOOCbJnn3225QEiWqtXr07NGxoaUvM8SZuce+211wbZVVddlbp+2p1Jf/az8IrgDz/8sIDRoVR+8pOfVHsIuZF2Z/U0Y8aMKfNIqo8zLwAAICo0LwAAICo0LwAAICo0LwAAICpM2M2RtEeyS9I///M/B9nBBx8cZGmTc9OkPapdkiZNmpRpfdSO8ePHV3sImfTq1SvI0ibifu1rXwuycePGpW7zjDPOKH5gQA49+uij1R5C2XHmBQAARIXmBQAARIXmBQAARIXmBQAARIUJuxWw1157Bdmll14aZF/96ldT1//sZz9b8L7XrVsXZM3dPXX9+vUF7wf5YmaZstNOOy11/cGDB5d8TFl95zvfCbJ///d/D7KOHTsG2ejRo4Ps3HPPLc3AAOQGZ14AAEBUaF4AAEBUaF4AAEBUaF4AAEBUWmxezGwnM5toZrPN7FUzG5zknc1sgpnNTT53Kv9wgfKi3lFvqHnEKMvVRmslXeHu082svaRpZjZB0vmSnnb3YWY2RNIQSVeXb6j5k3YVUP/+/YMs7cqi7t27l3w8U6dODbIbbrghyGK5JXyV1ES9u3umrLkr2W6//fYgGzFiRJC9++67QXbooYcG2cCBA4PsgAMOSN335z//+SB74403guzJJ58Msrvvvjt1m9ikmqj5epV2FeGee+6ZuuzLL79c7uFUTItnXty9wd2nJ1+vkDRb0o6S+kkalSw2SlL6NZdARKh31BtqHjFq1X1ezKy7pAMlTZbU1d0bpMbiN7MdmllnkKRBxQ0TqDzqHfWGmkcsMjcvZratpDGSLnf35WmnqtK4+3BJw5NthOetgRyi3lFvqHnEJNPVRmbWTo1FPdrdxybxYjPrlvy8m6Ql5RkiUFnUO+oNNY/YtHjmxRrb7/slzXb3W5v8aLyk8yQNSz6PK8sIK6xr166pec+ePYPszjvvDLK999675GOaPHlykP30pz8NsnHjwj8CbvnfOvVW723btk3NL7nkkiA744wzgmz58uVB1qNHj6LG9OKLLwbZxIkTg+y6664raj9oVG81X2vSJuK3aVP7d0HJ8rbREZIGSnrFzGYk2TVqLOhHzOxCSW9IOrM8QwQqinpHvaHmEZ0Wmxd3/7Ok5t78PLa0wwGqi3pHvaHmEaPaP7cEAABqCs0LAACISqvu8xKzzp07B9mvfvWrIOvVq1fq+rvttltJx5M2KfFnP/tZ6rJpdxJdtWpVSceD2vLSSy8F2ZQpU4Ls4IMPzrzNtLvxNjfBfWNpd+J9+OGHU5cdPHhw5jEBCB122GGp+ciRIys7kDLizAsAAIgKzQsAAIgKzQsAAIgKzQsAAIhK9BN2v/SlLwXZVVddFWSHHHJIkO24444lH89HH30UZLfffnuQ3XjjjUG2cuXKko8H9WnhwoVB9tWvfjXILr744tT1r7322oL3fdtttwXZPffcE2Tz5s0reB8AGmV9BlWt4cwLAACICs0LAACICs0LAACICs0LAACISvQTdk8//fRMWVazZs1KzX//+98H2dq1a4Ms7S65y5YtK3g8QKk0NDQE2dChQ1OXbS4HUD1PPPFEkJ15Zn0+7JszLwAAICo0LwAAICo0LwAAICo0LwAAIC7uvskPSTtJmihptqRXJQ1O8qGS3pI0I/nok2FbzgcfRX5MbanOivkQ9c5Hvj7KWu/UPB95+8hat1muNlor6Qp3n25m7SVNM7MJyc9+7u63ZNgGEAvqHfWGmkd0Wmxe3L1BUkPy9Qozmy2p9A8FAnKAeke9oeYRo1bNeTGz7pIOlDQ5iS41s7+a2Qgz69TMOoPMbKqZTS1qpECFUe+oN9Q8YmHJ+5QtL2i2raTnJN3g7mPNrKukd9T4PtX1krq5+zda2Ea2nQHNm+buvcu9E+odOVGRepeoeeSDu2d6THamMy9m1k7SGEmj3X1ssoPF7r7O3ddLulfSIYUOFsgT6h31hppHbFpsXszMJN0vaba739ok79ZksdMlzSz98IDKot5Rb6h5xCjL1UZHSBoo6RUzm5Fk10jqb2a91HhKcb6ki8syQqCyqHfUG2oe0ck856UkO+P9UBSvYnMAikW9owSiqXeJmkfxSjrnBQAAIC9oXgAAQFRoXgAAQFRoXgAAQFRoXgAAQFRoXgAAQFRoXgAAQFSy3KSulN6RtCD5ukvyfS2opWOR8n08u1R7AK1Qq/Uu1dbx5PlYYqp36R81n+ffaSE4nsrIXO8VvUndp3ZsNjWmmy9tSi0di1R7x5MHtfY7raXjqaVjyYta+51yPPnD20YAACAqNC8AACAq1Wxehldx36VWS8ci1d7x5EGt/U5r6Xhq6VjyotZ+pxxPzlRtzgsAAEAheNsIAABEheYFAABEpeLNi5mdZGZ/M7N5Zjak0vsvlpmNMLMlZjazSdbZzCaY2dzkc6dqjjErM9vJzCaa2Wwze9XMBid5lMeTR9R7vlDz5UfN50ct13tFmxczayvpLkknS+opqb+Z9azkGEpgpKSTNsqGSHra3XtIejr5PgZrJV3h7vtIOlTSt5I/j1iPJ1eo91yi5suIms+dmq33Sp95OUTSPHd/3d3XSHpYUr8Kj6Eo7j5J0nsbxf0kjUq+HiXptIoOqkDu3uDu05OvV0iaLWlHRXo8OUS95ww1X3bUfI7Ucr1XunnZUdKbTb5fmGSx6+ruDVJjsUjaocrjaTUz6y7pQEmTVQPHkxPUe45R82VBzedUrdV7pZsXS8m4VrvKzGxbSWMkXe7uy6s9nhpCvecUNV821HwO1WK9V7p5WShppybff17SogqPoRwWm1k3SUo+L6nyeDIzs3ZqLOrR7j42iaM9npyh3nOImi8raj5narXeK928TJHUw8x2NbPNJZ0taXyFx1AO4yWdl3x9nqRxVRxLZmZmku6XNNvdb23yoyiPJ4eo95yh5suOms+RWq73it9h18z6SPqFpLaSRrj7DRUdQJHM7CFJx6jxkeKLJf1A0mOSHpG0s6Q3JJ3p7htP+ModMztS0vOSXpG0PomvUeN7otEdTx5R7/lCzZcfNZ8ftVzvPB4AAABEhTvsAgCAqNC8AACAqNC8AACAqNC8AACAqNC8AACAqNC8AACAqNC8AACAqNC8AACAqNC8AACAqNC8AACAqNC8AACAqNC8AACAqNC8lJGZDTWzB6s9DqASqHfUE+q9umheimRmXzezqWb2oZk1mNkTyWPIqzGW683sFTNba2ZDqzEG1Lac1fvhZvbfZrbCzP5arXGgduWl3s1sBzN7yMwWmdkHZvaCmX2p0uPIE5qXIpjZdyX9QtKNkrpK2lnS3ZL6VWlI8yR9T9IfqrR/1LA81buZdZY0XtJPJW0n6WZJj5tZp0qPBbUpT/UuaVtJUyQdJKmzpFGS/mBm21ZhLLlA81IgM+so6UeSvuXuY919pbt/4u6Pu/tVzazzOzN7O+mcJ5nZvk1+1sfMZiX/i3zLzK5M8i5m9nszW2Zm75nZ82aW+ufm7qPc/QlJK8pwyKhjOaz3wyUtdvffufs6d39Q0lJJXy390aPe5K3e3f11d7/V3RuSeh8uaXNJe5XnN5B/NC+FO0zSlpIebcU6T0jqIWkHSdMljW7ys/slXezu7SXtJ+mZJL9C0kJJ26ux+79Gkhc1cqD18lbvlnxsnO3XivEBzclbvX+KmfVSY/MyrxXjqyk0L4X7jKR33H1t1hXcfYS7r3D31ZKGSjog6fAl6RNJPc2sg7u/7+7Tm+TdJO2SdP7PuzvNCyotb/X+oqTPmVl/M2tnZudJ2l3S1gUeH9BU3ur9/5hZB0m/kfRDd/+glcdVM2heCveupC5mtlmWhc2srZkNM7PXzGy5pPnJj7okn8+Q1EfSAjN7zswOS/KfqrG7fsrMXjezIaU7BCCzXNW7u7+rxrkH35W0WNJJkv6kxv/FAsXKVb032c9Wkh6X9LK739S6Q6otNC+Fe0nSx5JOy7j819X4YnucpI6Suie5SZK7T3H3fmo85fiYpEeSfIW7X+Huu0k6RdJ3zezYUh0EkFHu6t3dn3P3g929s6SBanz//78LODZgY7mrdzPbIln3LUkXF3BMNYXmpUDJ6brrJN1lZqeZ2dbJ6euTzezmlFXaS1qtxo5+azXOYJckmdnmZnaOmXV0908kLZe0LvlZXzPbw8ysSb4ubUzJ/rdU45/rZma2pZm1Ld1Ro17ltN4PTMbQQdItkha6+5OlO2rUq7zVu5m1k/SfklZJOtfd15f0gCNE81IEd79Vjaetr1XjlQ5vSrpUjd3xxh6QtECNXfMsSS9v9POBkuYnpxz/VdKAJO+hxtPhH6rxfwN3u/uzzQzpXjUWd39J30++HljAoQGBHNb79yS9k4yjm6TTCzkuIE3O6v1wSX0lnSBpmTXed+ZDM/t/BR9g5Iy5nwAAICaceQEAAFGheQEAAFGheQEAAFEpqnkxs5PM7G9mNo/7j6AeUPOoJ9Q78qrgCbvJJbh/l3S8Gm8MNUVSf3eftYl1mB2MYr3j7ttXY8etrXnqHSUQTb0n61DzKIq7b/zYj1TFnHk5RNK85IFRayQ9rOo9TRn1Y0EV903No9KodyBFMc3Ljmq87n2DhUn2KWY2yMymmtnUIvYF5EGLNU+9o4bwGo/cyvTchmakndoJThkmj+4eLnFKEdFrseapd9QQXuORW8WceVkoaacm339e0qLihgPkGjWPekK9I7eKaV6mSOphZrua2eaSzpY0vjTDAnKJmkc9od6RWwW/beTua83sUklPSmoraYS7v1qykQE5Q82jnlDvyLOKPtuI90NRAtPcvXe1B5EF9Y4SiKbeJWoexavEpdIAAAAVR/MCAACiQvMCAACiQvMCAACiQvMCAACiQvMCAACiQvMCAACiQvMCAACiQvMCAACiQvMCAACiQvMCAACiQvMCAACiQvMCAACiQvMCAACiQvMCAACiQvMCAACiQvMCAACislkxK5vZfEkrJK2TtNbde5diUCida6+9Nsh++MMfpi7bpk3Yyx5zzDFB9txzzxU9rlhR86gn1HtltG/fPsi23XbbIPvKV74SZNtvv32Q3Xrrran7Wb16dQGjy6eimpfEl939nRJsB4gFNY96Qr0jd3jbCAAARKXY5sUlPWVm08xsUNoCZjbIzKaa2dQi9wXkwSZrnnpHjeE1HrlU7NtGR7j7IjPbQdIEM5vj7pOaLuDuwyUNlyQz8yL3B1TbJmueekeN4TUeuVRU8+Lui5LPS8zsUUmHSJq06bVQLueff36QXX311UG2fv36zNt057WoKWoe9YR6L1z37t2DLO31WJIOO+ywINtvv/0K3ne3bt1S829/+9sFbzNvCn7byMy2MbP2G76WdIKkmaUaGJA31DzqCfWOPCvmzEtXSY+a2Ybt/Ie7/1dJRgXkEzWPekK9I7cKbl7c/XVJB5RwLECuUfOoJ9Q78ox5f5ylAAAgAElEQVRLpQEAQFRKcZM65MQuu+wSZFtuuWUVRoJ69qUvfSnIBgwYEGRHH3106vr77rtvpv1ceeWVQbZo0aIgO/LII1PXf/DBB4Ns8uTJmfaN+rX33nsH2eWXXx5k55xzTpBttdVWqdtM3pr7lDfffDPIVqxYEWT77LNPkJ111lmp+7n77ruDbM6cOanL5h1nXgAAQFRoXgAAQFRoXgAAQFRoXgAAQFSYsBup4447Lsguu+yyTOs2N0Grb9++QbZ48eLWDQx15Wtf+1qQ3XbbbUHWpUuXIEubpChJzz77bJBtv/32QfbTn/40wwib30/aNs8+++xM20Tt6dixY5D95Cc/CbK0mm/fvn1R+547d26QnXjiiUHWrl27IEt7PU/7+7apPEaceQEAAFGheQEAAFGheQEAAFGheQEAAFGheQEAAFHhaqMIpN3e/Ne//nWQpc2WT9PcVRoLFixo3cBQszbbLHxp6N27d5Dde++9Qbb11lsH2aRJk4Ls+uuvT933n//85yDbYostguyRRx4JshNOOCF1m2mmTp2aeVnUvtNPPz3I/uVf/qWk+3jttddS8+OPPz7I0h4PsMcee5R0PDHjzAsAAIgKzQsAAIgKzQsAAIgKzQsAAIhKixN2zWyEpL6Slrj7fknWWdJvJXWXNF/SWe7+fvmGWd/OO++8IPvc5z6Xad20W60/8MADxQ6pplHz0oABA4Lsvvvuy7TuhAkTgiztlurLly/PPJ609bNOzl24cGFqPmrUqMz7r2XUe6Mzzzyz4HXnz58fZFOmTAmyq6++OnX9tMm5afbZZ59WjauWZTnzMlLSSRtlQyQ97e49JD2dfA/UipGi5lE/Rop6R2RabF7cfZKk9zaK+0na8N+WUZJOK/G4gKqh5lFPqHfEqND7vHR19wZJcvcGM9uhuQXNbJCkQQXuB8iLTDVPvaNG8BqPXCv7Tercfbik4ZJkZl7u/QHVRL2j3lDzqIZCm5fFZtYt6ci7SVpSykHVqy5duqTm3/jGN4Js/fr1QbZs2bIg+/GPf1z8wCDVaM03d5fba665Jsjcw3+X7r777iC79tprg6w1k3PTfP/73y943W9/+9up+dKlSwveZh2oyXrflIsuuijIBg0KTyg99dRTQTZv3rwgW7Kk9L+yrl27lnybsSr0UunxkjZcAnOepHGlGQ6QW9Q86gn1jlxrsXkxs4ckvSRpLzNbaGYXShom6Xgzmyvp+OR7oCZQ86gn1Dti1OLbRu7ev5kfHVvisQC5QM2jnlDviBF32AUAAFEp+9VGSNe9e/cgGzNmTFHbvOOOO4Js4sSJRW0TteO6664LsrSJuZK0Zs2aIHvyySeDLO2OoatWrco0ni233DI1T7tz7s477xxkZhZkaRPUx41jugZatmjRoiAbOnRo5QeyCYcddli1h5AbnHkBAABRoXkBAABRoXkBAABRoXkBAABRYcJulZx00sYPcZX233//zOs//fTTQXbbbbcVNSbUju222y7ILrnkkiBLu2uulD4597TTCn823x577BFko0ePTl32oIMOyrTN//zP/wyym2++uXUDA8ok7c7O22yzTVHb/MIXvpBpuRdffDE1f+mll4raf55w5gUAAESF5gUAAESF5gUAAESF5gUAAESFCbsVkDbRcdiw7M85+/Of/xxk5513XpB98MEHrRsYatbmm28eZF26dMm8ftpkwx122CHILrjggiA79dRTg2y//fYLsm233TZ132mTiNOyBx98MMhWrlyZuk2gEFtvvXWQ9ezZM8h+8IMfBFmfPn0y76dNm/A8wvr16zOtm3Zn4LS/l5K0bt26zGPKO868AACAqNC8AACAqNC8AACAqNC8AACAqNC8AACAqLR4tZGZjZDUV9ISd98vyYZKukjS0mSxa9z9j+UaZEy6d+8eZGPGjClqm6+//nqQLV68uKhtonm1UPNr1qwJsqVLlwbZ9ttvn7r+//7v/wZZc48SyCLtiojly5enLtutW7cge+edd4Ls8ccfL3g8+IdaqPfWaNeuXWp+4IEHBlnaa3dafa5atSrI0mq+udvzpz0uJu1KpzSbbRb+M/7Vr341ddm0R8ikvVbEIMuZl5GSwt+s9HN375V81ERRA4mRouZRP0aKekdkWmxe3H2SpPcqMBYgF6h51BPqHTEqZs7LpWb2VzMbYWadmlvIzAaZ2VQzm1rEvoA8aLHmqXfUEF7jkVuFNi/3SNpdUi9JDZJ+1tyC7j7c3Xu7e+8C9wXkQaaap95RI3iNR64V9HgAd/+/2aJmdq+k35dsRJG7+uqrgyzrbZ6b05pHCaA8Yqv5ZcuWBVnaYyp+//v0w+jcuXOQvfbaa0E2bty4IBs5cmSQvfde+K7Eww8/nLrvtAmRzS2L8oit3puT9piMtMmxkjR27NhM2/zhD38YZM8880yQvfDCC0GW9vequfXTHqmRJm3S/U033ZS67BtvvBFkjz32WJCtXr06076rqaAzL2bW9NXldEkzSzMcIJ+oedQT6h15l+VS6YckHSOpi5ktlPQDSceYWS9JLmm+pIvLOEagoqh51BPqHTFqsXlx9/4p8f1lGAuQC9Q86gn1jhhxh10AABCVgibsolGvXr2C7IQTTih4e2mTHyXpb3/7W8HbBDaYPHlykDV3h91SO+qoo4Ls6KOPTl02bYJ72l2mgabS7pybNrn2qquuyrzNJ554IsjuuOOOIEubIJ/2d+uPf0y/198XvvCFIEu78+3NN98cZGkTe/v165e6n9GjRwfZn/70pyD7yU9+EmTvv/9+6jY3NmPGjEzLFYszLwAAICo0LwAAICo0LwAAICo0LwAAICpM2C3CU089FWSdOjX7CJBPefnll4Ps/PPPL3ZIQC5ttdVWQdbcnafdPci4wy6aatu2bZBdf/31QXbllVcG2cqVK1O3OWTIkCBLq7u0ybm9e4dPRrjzzjuD7MADD0zd99y5c4Psm9/8ZpBNnDgxyDp06BBkhx9+eOp+zjnnnCA79dRTg2zChAmp62/szTffDLJdd90107rF4swLAACICs0LAACICs0LAACICs0LAACIiqVNjivbzswqt7MKWLduXZA1NwlxY+eee26QPfTQQ0WPqQ5Mc/dwdlwO1Vq9l1ra3x8pfcJut27dgmzp0qUlH1MORVPvUuVqPm0ya9qdbz/66KMgGzRoUOo20y7A+NKXvhRkF1xwQZCdfPLJQZY2Sf1HP/pR6r5//etfB1naZNhy6N8/fLTV17/+9Uzrfuc73wmyefPmFTUed7csy3HmBQAARIXmBQAARIXmBQAARIXmBQAARKXF5sXMdjKziWY228xeNbPBSd7ZzCaY2dzkc7ZbywI5Rr2j3lDziFGLVxuZWTdJ3dx9upm1lzRN0mmSzpf0nrsPM7Mhkjq5+9UtbCvaqy/SZoOn3c4/69VGu+22W5AtWLCg1eOqQ2W9+oJ6L48TTzwxyP74xz+mLsvVRp9S9quNYqz5hoaGINt+++2DbPXq1UE2Z86c1G1us802QbbHHnsUMLpGQ4cODbKbbropddnmrryrRyW72sjdG9x9evL1CkmzJe0oqZ+kUclio9RY7EDUqHfUG2oeMWrVnBcz6y7pQEmTJXV19wapsfgl7VDqwQHVRL2j3lDziEXmp0qb2baSxki63N2Xm2U6syMzGyQp/a5AQE5R76g31DxikunMi5m1U2NRj3b3sUm8OHmvdMN7pkvS1nX34e7eO6a7RKK+Ue+oN9Q8YtPimRdrbL/vlzTb3W9t8qPxks6TNCz5PK4sI6ywXr16pebHHXdckKVNzl2zZk2Q3XXXXUG2ePHiAkaHcqu3eq+UtAnqyIcYa/7tt98OsrQJu1tssUWQHXDAAZn3kzapfNKkSUH22GOPBdn8+fODjIm5pZPlbaMjJA2U9IqZzUiya9RY0I+Y2YWS3pB0ZnmGCFQU9Y56Q80jOi02L+7+Z0nNvfl5bGmHA1QX9Y56Q80jRtxhFwAARIXmBQAARCXzpdL1YrvttkvNP/vZz2Za/6233gqyK6+8sqgxAbF7/vnng6xNm/T/O2W9SzXq11FHHRVkp50W3kPvi1/8YpAtWZJ60ZRGjBgRZO+//36QpV2UgcrjzAsAAIgKzQsAAIgKzQsAAIgKzQsAAIgKE3YBlN3MmTODbO7cuanLpt2Nd/fddw+ypUuXFj8wRGnFihVB9pvf/CZThtrAmRcAABAVmhcAABAVmhcAABAVmhcAABAVJuxuZM6cOan5iy++GGRHHnlkuYcD1Kwbb7wxNb/vvvuC7IYbbgiyyy67LMhmzZpV/MAA5B5nXgAAQFRoXgAAQFRoXgAAQFRoXgAAQFTM3Te9gNlOkh6Q9FlJ6yUNd/fbzGyopIskbbjN5TXu/scWtrXpnQEtm+buvcu1ceq9cjp06JCaP/LII0F23HHHBdnYsWOD7IILLgiylStXFjC63ChrvUvUPPLF3S3LclmuNlor6Qp3n25m7SVNM7MJyc9+7u63FDpIIIeod9Qbah7RabF5cfcGSQ3J1yvMbLakHcs9MKAaqHfUG2oeMWrVnBcz6y7pQEmTk+hSM/urmY0ws07NrDPIzKaa2dSiRgpUGPWOekPNIxaZmxcz21bSGEmXu/tySfdI2l1SLzV27T9LW8/dh7t773K/bwuUEvWOekPNIyaZmhcza6fGoh7t7mMlyd0Xu/s6d18v6V5Jh5RvmEDlUO+oN9Q8YtPinBczM0n3S5rt7rc2ybsl75VK0umSZpZniEDlUO+Vs3z58tT8rLPOCrK0xwN885vfDLKhQ4cGGY8M2DRqHjHKcrXREZIGSnrFzGYk2TWS+ptZL0kuab6ki8syQqCyqHfUG2oe0clytdGfJaVdd73J6/2BGFHvqDfUPGLEHXYBAEBUaF4AAEBUWnw8QEl3xq2jUbyy3y69VKh3lEA09S5R8yhe1scDcOYFAABEheYFAABEheYFAABEheYFAABEJctN6krpHUkLkq+7JN/Xglo6Finfx7NLtQfQCrVa71JtHU+ejyWmepf+UfN5/p0WguOpjMz1XtGrjT61Y7OpMc2i35RaOhap9o4nD2rtd1pLx1NLx5IXtfY75Xjyh7eNAABAVGheAABAVKrZvAyv4r5LrZaORaq948mDWvud1tLx1NKx5EWt/U45npyp2pwXAACAQvC2EQAAiArNCwAAiErFmxczO8nM/mZm88xsSKX3XywzG2FmS8xsZpOss5lNMLO5yedO1RxjVma2k5lNNLPZZvaqmQ1O8iiPJ4+o93yh5suPms+PWq73ijYvZtZW0l2STpbUU1J/M+tZyTGUwEhJJ22UDZH0tLv3kPR08n0M1kq6wt33kXSopG8lfx6xHk+uUO+5RM2XETWfOzVb75U+83KIpHnu/rq7r5H0sKR+FR5DUdx9kqT3Nor7SRqVfD1K0mkVHVSB3L3B3acnX6+QNFvSjor0eHKIes8Zar7sqPkcqeV6r3TzsqOkN5t8vzDJYtfV3RukxmKRtEOVx9NqZtZd0oGSJqsGjicnqPcco+bLgprPqVqr90o3L5aSca12lZnZtpLGSLrc3ZdXezw1hHrPKWq+bKj5HKrFeq9087JQ0k5Nvv+8pEUVHkM5LDazbpKUfF5S5fFkZmbt1FjUo919bBJHezw5Q73nEDVfVtR8ztRqvVe6eZkiqYeZ7Wpmm0s6W9L4Co+hHMZLOi/5+jxJ46o4lszMzCTdL2m2u9/a5EdRHk8OUe85Q82XHTWfI7Vc7xW/w66Z9ZH0C0ltJY1w9xsqOoAimdlDko5R4yPFF0v6gaTHJD0iaWdJb0g60903nvCVO2Z2pKTnJb0iaX0SX6PG90SjO548ot7zhZovP2o+P2q53nk8AAAAiAp32AUAAFGheQEAAFGheQEAAFGheQEAAFGheQEAAFGheQEAAFGheQEAAFGheQEAAFGheQEAAFGheQEAAFGheQEAAFGheQEAAFGheSkjMxtqZg9WexxAJVDvqCfUe3XRvBTJzL5uZlPN7EMzazCzJ5LHkFdjLNeb2StmttbMhlZjDKhtOav3iWa21MyWm9lfzKxfNcaB2pWzeuf1vQmalyKY2Xcl/ULSjZK6StpZ0t2SqvUiOk/S9yT9oUr7Rw3LYb0PltTN3TtIGiTpQTPrVqWxoMbksN55fW+C5qVAZtZR0o8kfcvdx7r7Snf/xN0fd/ermlnnd2b2tpl9YGaTzGzfJj/rY2azzGyFmb1lZlcmeRcz+72ZLTOz98zseTNL/XNz91Hu/oSkFWU4ZNSxnNb7X9197YZvJbWTtFNJDxx1Kaf1zut7EzQvhTtM0paSHm3FOk9I6iFpB0nTJY1u8rP7JV3s7u0l7SfpmSS/QtJCSdursfu/Ro0v1EAl5bLekxf+jyVNlvSspKmtGB/QnFzWO/5hs2oPIGKfkfROk//5tcjdR2z4OnnP8n0z6+juH0j6RFJPM/uLu78v6f1k0U8kdZO0i7vPk/R8qQ4AaIVc1ru79zWzdpKOk7S3u69vzUEBzchlveMfOPNSuHcldTGzTA2gmbU1s2Fm9pqZLZc0P/lRl+TzGZL6SFpgZs+Z2WFJ/lM1vtf5lJm9bmZDSncIQGa5rffkdP4Tkk40s1NbcUxAc3Jb72hE81K4lyR9LOm0jMt/XY0TvY6T1FFS9yQ3SXL3Ke7eT42nHB+T9EiSr3D3K9x9N0mnSPqumR1bqoMAMoqh3jeTtHvGZYFNiaHe6xrNS4GSU4HXSbrLzE4zs63NrJ2ZnWxmN6es0l7SajV29FurcQa7JMnMNjezc5JTjJ9IWi5pXfKzvma2h5lZk3xd2piS/W+pxj/XzcxsSzNrW7qjRr3KW72b2d7JvrdKxjFA0lGSnivtkaMe5a3ek2V5fW+C5qUI7n6rpO9KulbSUklvSrpUjZ31xh6QtEDSW5JmSXp5o58PlDQ/OeX4r5IGJHkPSX+S9KEa/zdwt7s/28yQ7pW0SlJ/Sd9Pvh5YwKEBgZzVu0kaKmlJMpbBkr7m7tMLOzrg03JW7xKv759i7kxsBgAA8eDMCwAAiArNCwAAiArNCwAAiArNCwAAiEpRd9g1s5Mk3SapraT73H1YC8szOxjFesfdt6/WzltT89Q7SiCaek+Wp+ZRFHe3LMsVfOYlub78LkknS+opqb+Z9Sx0e0BGC6q1Y2oeVUC9AymKedvoEEnz3P11d18j6WFV71HhQCVQ86gn1Dtyq5jmZUc13rRng4VJ9ilmNsjMppoZT3tF7FqseeodNYTXeORWMXNe0t6XCt7vdPfhkoZLvB+K6LVY89Q7agiv8citYs68LJS0U5PvPy9pUXHDAXKNmkc9od6RW8U0L1Mk9TCzXc1sc0lnSxpfmmEBuUTNo55Q78itgt82cve1ZnappCfVeBndCHd/tWQjA3KGmkc9od6RZxV9MCPvh6IEprl772oPIgvqHSUQTb1L1DyKV/b7vAAAAFQDzQsAAIgKzQsAAIgKzQsAAIgKzQsAAIgKzQsAAIgKzQsAAIgKzQsAAIgKzQsAAIgKzQsAAIgKzQsAAIgKzQsAAIhKwU+VRna33XZbkH37298OspkzZ6au37dv3yBbsGBB8QMDACBCnHkBAABRoXkBAABRoXkBAABRKWrOi5nNl7RC0jpJa929dykGBeQVNY96Qr0jr0oxYffL7v5OCbZTE7p37x5kAwYMCLL169cH2T777JO6zb333jvImLBbVdR8Ys899wyydu3aBdlRRx0VZHfffXfqNtP+bpTDuHHjguzss88OsjVr1lRiOHlGvbcgreYPP/zwILvxxhtT1z/iiCNKPqZax9tGAAAgKsU2Ly7pKTObZmaDSjEgIOeoedQT6h25VOzbRke4+yIz20HSBDOb4+6Tmi6QFDxFj1qxyZqn3lFjeI1HLhV15sXdFyWfl0h6VNIhKcsMd/feTPRCLWip5ql31BJe45FXBZ95MbNtJLVx9xXJ1ydI+lHJRhappUuXBtmkSZOC7NRTT63EcFBC9VTz++67b5Cdf/75QXbmmWcGWZs24f+JPve5zwVZcxNz3T3DCIuX9nfwl7/8ZZBdfvnlQbZ8+fKyjClP6qnei9WxY8cgmzhxYpC9/fbbqet/9rOfzbwsGhXztlFXSY+a2Ybt/Ie7/1dJRgXkEzWPekK9I7cKbl7c/XVJB5RwLECuUfOoJ9Q78oxLpQEAQFRoXgAAQFRKcYddNLFy5cog4264iM1NN90UZH369KnCSCrr3HPPDbL7778/yF544YVKDAc1Jm1ibnM5E3Y3jTMvAAAgKjQvAAAgKjQvAAAgKjQvAAAgKkzYLbHtttsuyA44gFslIC4TJkwIsqwTdpcsWRJkaZNe0+7EKzV/592NHX744UF29NFHZ1oXqIbkhn8oAc68AACAqNC8AACAqNC8AACAqNC8AACAqNC8AACAqHC1UYltvfXWQbbzzjsXtc2DDz44yObMmRNkPIYApXLPPfcE2WOPPZZp3U8++STIynGr8w4dOgTZzJkzg+xzn/tc5m2mHePUqVNbNzCgGe6emm+55ZYVHkn8OPMCAACiQvMCAACiQvMCAACi0mLzYmYjzGyJmc1sknU2swlmNjf53Km8wwQqh5pHPaHeEaMsE3ZHSrpT0gNNsiGSnnb3YWY2JPn+6tIPLz6LFi0KspEjRwbZ0KFDM28zbdlly5YF2Z133pl5m9ikkarzml+7dm2Qvfnmm1UYSfNOPPHEIOvUqbh/YxcuXBhkq1evLmqbERipOq/3auvdu3eQvfzyy1UYSTxaPPPi7pMkvbdR3E/SqOTrUZJOK/G4gKqh5lFPqHfEqNBLpbu6e4MkuXuDme3Q3IJmNkjSoAL3A+RFppqn3lEjeI1HrpX9Pi/uPlzScEkys/SL3IEaQb2j3lDzqIZCrzZabGbdJCn5vKR0QwJyiZpHPaHekWuFnnkZL+k8ScOSz+NKNqIadP311wdZaybsIheo+So6++yzg+yiiy4Ksq222qqo/Vx33XVFrV9DqPdWSJvg/sEHHwRZx44dU9fffffdSz6mWpflUumHJL0kaS8zW2hmF6qxoI83s7mSjk++B2oCNY96Qr0jRi2eeXH3/s386NgSjwXIBWoe9YR6R4y4wy4AAIgKzQsAAIhK2S+VRro2bcK+cf369VUYCVAd55xzTmo+ZMiQINtjjz2CrF27dkXtf8aMGUH2ySefFLVN1Ke0O54///zzQda3b99KDKcucOYFAABEheYFAABEheYFAABEheYFAABEhQm7VZI2Odedx4IgH7p37x5kAwcODLLjjjuu4H0ceeSRqXkxfw+WL18eZGkTgCXpj3/8Y5CtWrWq4H0DqBzOvAAAgKjQvAAAgKjQvAAAgKjQvAAAgKgwYReoY/vtt19qPn78+CDbeeedyz2coqXd1XT48OFVGAmQ3Wc+85lqDyE6nHkBAABRoXkBAABRoXkBAABRoXkBAABRabF5MbMRZrbEzGY2yYaa2VtmNiP56FPeYQKVQ82jnlDviFGWq41GSrpT0gMb5T9391tKPiKg+kaqzmvezDJlxWjTJv3/TmmPzsiqb9++QXbyySenLvvEE08UvJ8aM1J1Xu/Vduqpp1Z7CNFp8cyLu0+S9F4FxgLkAjWPekK9I0bFzHm51Mz+mpxy7NTcQmY2yMymmtnUIvYF5EGLNU+9o4bwGo/cKrR5uUfS7pJ6SWqQ9LPmFnT34e7e2917F7gvIA8y1Tz1jhrBazxyraDmxd0Xu/s6d18v6V5Jh5R2WEC+UPOoJ9Q78q6gxwOYWTd3b0i+PV3SzE0tj1DaZMXWTFQ86qijguzOO+8sakxoXq3W/MyZ6YdxzDHHBNmAAQOC7Mknnwyyjz/+uOhxbezCCy8Msssuu6zk+0GjWq33Spo4cWKQpU0oR2FabF7M7CFJx0jqYmYLJf1A0jFm1kuSS5ov6eIyjhGoKGoe9YR6R4xabF7cvX9KfH8ZxgLkAjWPekK9I0bcYRcAAESF5gUAAETF3L1yOzOr3M5ybt26dUFW7J/F/vvvH2SzZs0qaps5NC2WSzKp99Lp2LFjkL377ruZ1j3llFNS80jusBtNvUvUfFNnnHFGkP3ud79LXXbVqlVB1rNnzyBbsGBB8QPLOXfPdCtvzrwAAICo0LwAAICo0LwAAICo0LwAAICoFHSHXRTvl7/8ZZBdfHFx94EaNGhQkF1++eVFbRPIgxNPPLHaQwBaZe3atZmXNQvnqG6xxRalHE7N4cwLAACICs0LAACICs0LAACICs0LAACIChN2q2TOnDnVHgJqWLt27YLshBNOCLJnnnkmdf20O35WygUXXBBkt912WxVGAhRu3LhxQdbc6/7ee+8dZGkXW1xyySXFD6xGcOYFAABEheYFAABEheYFAABEheYFAABEpcXmxcx2MrOJZjbbzF41s8FJ3tnMJpjZ3ORzp/IPFygv6h31hppHjMzdN72AWTdJ3dx9upm1lzRN0mmSzpf0nrsPM7Mhkjq5+9UtbGvTO6tzf//731Pz3XffPdP6bdqEvegee+wRZK+99lrrBpYv09y9d7k2HmO9H3nkkUH2/e9/P8iOP/74INt1111Tt/nmm28WP7AmOnfuHGR9+vRJXfaOO+4Isvbt22faT9pVUqeeemrqshMnTsy0zSora71LcdZ8rH7xi1+k5mlX2HXt2jXIPv7445KPKW/cPXxWQooWz7y4e4O7T0++XiFptqQdJfWTNCpZbJQaix2IGvWOekPNI0atus+LmXWXdKCkyZK6unuD1Fj8ZrZDM+sMkhQ+MRDIOeod9YaaRywyNy9mtq2kMZIud/flaU/BTOPuwyUNT7bBKUVEgXpHvaHmEZNMVxuZWTs1FvVodx+bxIuT90o3vGe6pDxDBCqLeke9oeYRmxbPvFhj+32/pNnufmuTH42XdJ6kYcnn8F7IaJVXX301Nd9tt90yrcBGY6oAAAZdSURBVL9+/fpSDqcuxVjvd955Z5Dtt99+mdb93ve+l5qvWLGiqDFtLG2y8Be/+MXUZVu6iGCDZ599NsjuueeeIItkYm7VxFjztSat5tesWVOFkcQjy9tGR0gaKOkVM5uRZNeosaAfMbMLJb0h6czyDBGoKOod9YaaR3RabF7c/c+Smnvz89jSDgeoLuod9YaaR4y4wy4AAIgKzQsAAIhKq+7zgvIaPnx4an7KKadUeCSoF9/85jerPYTAkiXhRS2PP/54kA0ePDjI6uEOpKg9HTp0CLJ+/foF2aOPPlqJ4USBMy8AACAqNC8AACAqNC8AACAqNC8AACAqTNjNkVmzZqXms2fPDrJ99tmn3MNBJM4///wgu+yyy4LsvPPOq8BopNdeey3IPvrooyB7/vnnU9dPm7g+c+bM4gcGVNlZZ52Vmq9evTrI0l738Q+ceQEAAFGheQEAAFGheQEAAFGheQEAAFFhwm6OLFiwIDX/whe+UOGRICYzZswIsksuuSTI/vu//zvIfvzjH6dus1OnTkH22GOPBdmECROCbNy4cUH29ttvp+4HqCeTJk1KzdMuwFi1alW5hxM1zrwAAICo0LwAAICo0LwAAICo0LwAAIC4uPsmPyTtJGmipNmSXpU0OMmHSnpL0ozko0+GbTkffBT5MbWlOivmQ9Q7H/n6KGu9U/N85O0ja91mudporaQr3H26mbWXNM3MNlxi8HN3vyXDNoBYUO+oN9Q8otNi8+LuDZIakq9XmNlsSTuWe2BANVDvqDfUPGLUqjkvZtZd0oGSJifRpWb2VzMbYWbhjSEa1xlkZlPNbGpRIwUqjHpHvaHmEQtL3qdseUGzbSU9J+kGdx9rZl0lvaPG96mul9TN3b/Rwjay7Qxo3jR3713unVDvyImK1LtEzSMf3N2yLJfpzIuZtZM0RtJodx+b7GCxu69z9/WS7pV0SKGDBfKEeke9oeYRmxabFzMzSfdLmu3utzbJuzVZ7HRJM0s/PKCyqHfUG2oeMcpytdERkgZKesXMNjxE5RpJ/c2slxpPKc6XdHFZRghUFvWOekPNIzqZ57yUZGe8H4riVWwOQLGod5RANPUuUfMoXknnvAAAAOQFzQsAAIgKzQsAAIgKzQsAAIgKzQsAAIgKzQsAAIgKzQsAAIhKlpvUldI7khYkX3dJvq8FtXQsUr6PZ5dqD6AVarXepdo6njwfS0z1Lv2j5vP8Oy0Ex1MZmeu9ojep+9SOzabGdPOlTamlY5Fq73jyoNZ+p7V0PLV0LHlRa79Tjid/eNsIAABEheYFAABEpZrNy/Aq7rvUaulYpNo7njyotd9pLR1PLR1LXtTa75TjyZmqzXkBAAAoBG8bAQCAqNC8AACAqFS8eTGzk8zsb2Y2z8yGVHr/xTKzEWa2xMxmNsk6m9kEM5ubfO5UzTFmZWY7mdlEM5ttZq+a2eAkj/J48oh6zxdqvvyo+fyo5XqvaPNiZm0l3SXpZEk9JfU3s56VHEMJjJR00kbZEElPu/v/b+8OXasKwwCMPy+iyeyQqWhYWLSIwWC2zCLY9gdYBNuKySr+AwoLogiKrls0iWAxrIhBh2MGgzYRX8M94TIM03G++36H51d2znfL93Gf8LK73bMCvBzue/ALuJWZq8BF4MbwfvR6nlLsvSSbH5HNlzPZ3lv/5uUC8CEzP2bmT+AxsNZ4D4eSma+Ab/uW14DN4XoTuNp0U/8pM3cz891w/QPYBpbp9DwF2XsxNj86my9kyr23Hl6Wgc9z9zvDWu+WMnMXZrEAJxa8n38WEWeB88AbJnCeIuy9MJsfhc0XNbXeWw8v8Zc1/1d7wSLiOPAUuJmZ3xe9nwmx96JsfjQ2X9AUe289vOwAp+fuTwFfGu9hDHsRcRJg+Pl1wfs5sIg4yizqh5n5bFju9jzF2HtBNj8qmy9mqr23Hl7eAisRcS4ijgHXga3GexjDFrA+XK8DLxa4lwOLiADuA9uZeXfupS7PU5C9F2Pzo7P5Qqbce/Nv2I2IK8A94AjwIDPvNN3AIUXEI+Ays0eK7wG3gefAE+AM8Am4lpn7/+CrnIi4BLwG3gO/h+UNZp+Jdneeiuy9Fpsfn83XMeXefTyAJEnqit+wK0mSuuLwIkmSuuLwIkmSuuLwIkmSuuLwIkmSuuLwIkmSuuLwIkmSuvIHPLaU/dGM/pcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 576x576 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig=plt.figure(figsize=(8, 8))\n",
    "for i in range(9):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.title(\"Class {}\".format(y_train[i]))\n",
    "    plt.imshow(X_train[i], cmap='gray', interpolation='none')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pixel values are gray scale between 0 and 255. It is almost always a good idea to perform some scaling of input values when using neural network models. Because the scale is well known and well behaved, we can very quickly normalize the pixel values to the range 0 and 1 by dividing each value by the maximum of 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize inputs from 0-255 to 0-1\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the output variable is an integer from 0 to 9. As this is a multi-class classification problem we need to one hot encoding of the class values, transforming the vector of class integers into a binary matrix.\n",
    "\n",
    "We can easily do this using the built-in np_utils.to_categorical() helper function in Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,)\n",
      "(10000,)\n",
      "[5 0 4 1 9 2 1 3 1 4]\n",
      "(60000, 10)\n",
      "(10000, 10)\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# one hot encode outputs and save original classes\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "print(y_train[:10])\n",
    "y_train_classes = y_train.copy()\n",
    "y_test_classes = y_test.copy()\n",
    "y_train = np_utils.to_categorical(y_train_classes)\n",
    "y_test = np_utils.to_categorical(y_test_classes)\n",
    "num_classes = y_test.shape[1]\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "print(y_train[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means with 10 clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training dataset is structured as a 3-dimensional array of (instance, image width and image height). Our neural-network is going to take a single vector for each training example, so we need to reshape the input so that each 28x28 image becomes a single 784 dimensional vector.\n",
    "\n",
    "We can do this transform easily using the reshape() function on the NumPy array. We can also reduce our memory requirements by forcing the precision of the pixel values to be 32 bit, the default precision used by Keras anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "# flatten 28*28 images to a 784 vector for each image\n",
    "num_pixels = X_train.shape[1] * X_train.shape[2]\n",
    "X_train_reshaped = X_train.reshape(X_train.shape[0], num_pixels).astype('float32')\n",
    "X_test_reshaped = X_test.reshape(X_test.shape[0], num_pixels).astype('float32')\n",
    "print(X_train_reshaped.shape)\n",
    "print(X_test_reshaped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_jobs=-1, n_clusters=10, n_init=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "    n_clusters=10, n_init=20, n_jobs=-1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km.fit(X_train_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = km.predict(X_test_reshaped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Normalized Mutual Information (NMI) score to evaluate our model.\n",
    "Mutual information is a symmetric measure for the degree of dependency between the clustering and the manual classification. It is based on the notion of cluster purity pi, which measures the quality of a single cluster Ci, the largest number of objects in cluster Ci which Ci has in common with a manual class Mj, having compared Ci to all manual classes in M. Because NMI is normalized, we can use it to compare clusterings with different numbers of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49920798984923836"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_mutual_info_score(y_test_classes, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder\n",
    "Instead of directly applying K-Means on the dataset, we will first use an autoencoder to decrease the dimensionality of the data and extract useful information. We will then use this embedding to pass on to the K-Means algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is our input placeholder\n",
    "input_img = Input(shape=(784,))\n",
    "\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded = Dense(500, activation='relu')(input_img)\n",
    "encoded = Dense(500, activation='relu')(encoded)\n",
    "encoded = Dense(2000, activation='relu')(encoded)\n",
    "encoded = Dense(10, activation='sigmoid')(encoded)\n",
    "\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded = Dense(2000, activation='relu')(encoded)\n",
    "decoded = Dense(500, activation='relu')(decoded)\n",
    "decoded = Dense(500, activation='relu')(decoded)\n",
    "decoded = Dense(784)(decoded)\n",
    "\n",
    "#  this model maps an input to its encoded representation\n",
    "encoder = Model(input_img, encoded)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 500)               392500    \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 2000)              1002000   \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 10)                20010     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 2000)              22000     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 500)               1000500   \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 784)               392784    \n",
      "=================================================================\n",
      "Total params: 3,330,794\n",
      "Trainable params: 3,330,794\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(autoencoder.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 16s 271us/step - loss: 0.0153 - val_loss: 0.0157\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 16s 266us/step - loss: 0.0152 - val_loss: 0.0158\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 16s 259us/step - loss: 0.0151 - val_loss: 0.0158\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 16s 269us/step - loss: 0.0151 - val_loss: 0.0155\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 15s 258us/step - loss: 0.0150 - val_loss: 0.0155\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 15s 255us/step - loss: 0.0150 - val_loss: 0.0155\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 16s 261us/step - loss: 0.0148 - val_loss: 0.0154\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 16s 261us/step - loss: 0.0149 - val_loss: 0.0155\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 17s 287us/step - loss: 0.0148 - val_loss: 0.0153\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 17s 276us/step - loss: 0.0147 - val_loss: 0.0153\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 17s 290us/step - loss: 0.0147 - val_loss: 0.0152\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 16s 270us/step - loss: 0.0145 - val_loss: 0.0152\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 16s 274us/step - loss: 0.0145 - val_loss: 0.0151\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 17s 281us/step - loss: 0.0146 - val_loss: 0.0152\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 16s 267us/step - loss: 0.0144 - val_loss: 0.0150\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 16s 266us/step - loss: 0.0143 - val_loss: 0.0150\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 16s 259us/step - loss: 0.0144 - val_loss: 0.0151\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 15s 254us/step - loss: 0.0143 - val_loss: 0.0149\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 15s 252us/step - loss: 0.0143 - val_loss: 0.0149\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 16s 266us/step - loss: 0.0142 - val_loss: 0.0148\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 15s 256us/step - loss: 0.0141 - val_loss: 0.0148\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 15s 248us/step - loss: 0.0142 - val_loss: 0.0148\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 16s 261us/step - loss: 0.0140 - val_loss: 0.0147\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 16s 275us/step - loss: 0.0140 - val_loss: 0.0147\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 15s 252us/step - loss: 0.0139 - val_loss: 0.0147\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 15s 255us/step - loss: 0.0139 - val_loss: 0.0151\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 15s 256us/step - loss: 0.0139 - val_loss: 0.0146\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 15s 256us/step - loss: 0.0138 - val_loss: 0.0146\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 16s 261us/step - loss: 0.0138 - val_loss: 0.0148\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 16s 263us/step - loss: 0.0138 - val_loss: 0.0146\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 15s 254us/step - loss: 0.0136 - val_loss: 0.0145\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 15s 256us/step - loss: 0.0137 - val_loss: 0.0145\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 16s 259us/step - loss: 0.0136 - val_loss: 0.0145\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 16s 259us/step - loss: 0.0136 - val_loss: 0.0145\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 15s 252us/step - loss: 0.0135 - val_loss: 0.0144\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 15s 253us/step - loss: 0.0136 - val_loss: 0.0144\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 15s 250us/step - loss: 0.0134 - val_loss: 0.0143\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 15s 258us/step - loss: 0.0134 - val_loss: 0.0143\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 16s 263us/step - loss: 0.0134 - val_loss: 0.0143\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 17s 280us/step - loss: 0.0134 - val_loss: 0.0145\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 16s 268us/step - loss: 0.0134 - val_loss: 0.0142\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 16s 272us/step - loss: 0.0133 - val_loss: 0.0142\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 16s 267us/step - loss: 0.0132 - val_loss: 0.0142\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 17s 283us/step - loss: 0.0132 - val_loss: 0.0142\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 18s 298us/step - loss: 0.0132 - val_loss: 0.0141\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 18s 306us/step - loss: 0.0132 - val_loss: 0.0141\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 17s 280us/step - loss: 0.0131 - val_loss: 0.0142\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 16s 272us/step - loss: 0.0132 - val_loss: 0.0142\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 16s 265us/step - loss: 0.0130 - val_loss: 0.0141\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 16s 267us/step - loss: 0.0130 - val_loss: 0.0141\n"
     ]
    }
   ],
   "source": [
    "history = autoencoder.fit(X_train_reshaped, X_train_reshaped, epochs=50, batch_size=2048, validation_data=(X_test_reshaped, X_test_reshaped))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the embedded format for the training set using only the encoder part of the network and train the K-Means using this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "    n_clusters=10, n_init=20, n_jobs=-1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_auto_train = encoder.predict(X_train_reshaped)\n",
    "km.fit(pred_auto_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict using the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_auto = encoder.predict(X_test_reshaped)\n",
    "pred = km.predict(pred_auto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7414710645948941"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_mutual_info_score(y_test_classes, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEC\n",
    "State-of-the-art model â€“ known as DEC algorithm. This algorithm trains both clustering and autoencoder models to get better performance.\n",
    "DEC Code from: https://github.com/XifengGuo/DEC-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate clustering accuracy. Require scikit-learn installed\n",
    "    # Arguments\n",
    "        y: true labels, numpy.array with shape `(n_samples,)`\n",
    "        y_pred: predicted labels, numpy.array with shape `(n_samples,)`\n",
    "    # Return\n",
    "        accuracy, in [0,1]\n",
    "    \"\"\"\n",
    "    y_true = y_true.astype(np.int64)\n",
    "    assert y_pred.size == y_true.size\n",
    "    D = max(y_pred.max(), y_true.max()) + 1\n",
    "    w = np.zeros((D, D), dtype=np.int64)\n",
    "    for i in range(y_pred.size):\n",
    "        w[y_pred[i], y_true[i]] += 1\n",
    "    from sklearn.utils.linear_assignment_ import linear_assignment\n",
    "    ind = linear_assignment(w.max() - w)\n",
    "    return sum([w[i, j] for i, j in ind]) * 1.0 / y_pred.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Keras implementation for Deep Embedded Clustering (DEC) algorithm:\n",
    "\n",
    "Original Author:\n",
    "    Xifeng Guo. 2017.1.30\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def autoencoder(dims, act='relu', init='glorot_uniform'):\n",
    "    \"\"\"\n",
    "    Fully connected auto-encoder model, symmetric.\n",
    "    Arguments:\n",
    "        dims: list of number of units in each layer of encoder. dims[0] is input dim, dims[-1] is units in hidden layer.\n",
    "            The decoder is symmetric with encoder. So number of layers of the auto-encoder is 2*len(dims)-1\n",
    "        act: activation, not applied to Input, Hidden and Output layers\n",
    "    return:\n",
    "        (ae_model, encoder_model), Model of autoencoder and model of encoder\n",
    "    \"\"\"\n",
    "    n_stacks = len(dims) - 1\n",
    "    # input\n",
    "    x = Input(shape=(dims[0],), name='input')\n",
    "    h = x\n",
    "\n",
    "    # internal layers in encoder\n",
    "    for i in range(n_stacks-1):\n",
    "        h = Dense(dims[i + 1], activation=act, kernel_initializer=init, name='encoder_%d' % i)(h)\n",
    "\n",
    "    # hidden layer\n",
    "    h = Dense(dims[-1], kernel_initializer=init, name='encoder_%d' % (n_stacks - 1))(h)  # hidden layer, features are extracted from here\n",
    "\n",
    "    y = h\n",
    "    # internal layers in decoder\n",
    "    for i in range(n_stacks-1, 0, -1):\n",
    "        y = Dense(dims[i], activation=act, kernel_initializer=init, name='decoder_%d' % i)(y)\n",
    "\n",
    "    # output\n",
    "    y = Dense(dims[0], kernel_initializer=init, name='decoder_0')(y)\n",
    "\n",
    "    return Model(inputs=x, outputs=y, name='AE'), Model(inputs=x, outputs=h, name='encoder')\n",
    "\n",
    "\n",
    "class ClusteringLayer(Layer):\n",
    "    \"\"\"\n",
    "    Clustering layer converts input sample (feature) to soft label, i.e. a vector that represents the probability of the\n",
    "    sample belonging to each cluster. The probability is calculated with student's t-distribution.\n",
    "\n",
    "    # Example\n",
    "    ```\n",
    "        model.add(ClusteringLayer(n_clusters=10))\n",
    "    ```\n",
    "    # Arguments\n",
    "        n_clusters: number of clusters.\n",
    "        weights: list of Numpy array with shape `(n_clusters, n_features)` witch represents the initial cluster centers.\n",
    "        alpha: parameter in Student's t-distribution. Default to 1.0.\n",
    "    # Input shape\n",
    "        2D tensor with shape: `(n_samples, n_features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(n_samples, n_clusters)`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_clusters, weights=None, alpha=1.0, **kwargs):\n",
    "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
    "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
    "        super(ClusteringLayer, self).__init__(**kwargs)\n",
    "        self.n_clusters = n_clusters\n",
    "        self.alpha = alpha\n",
    "        self.initial_weights = weights\n",
    "        self.input_spec = InputSpec(ndim=2)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 2\n",
    "        input_dim = input_shape[1]\n",
    "        self.input_spec = InputSpec(dtype=K.floatx(), shape=(None, input_dim))\n",
    "        self.clusters = self.add_weight((self.n_clusters, input_dim), initializer='glorot_uniform', name='clusters')\n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "            del self.initial_weights\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        \"\"\" student t-distribution, as same as used in t-SNE algorithm.\n",
    "                 q_ij = 1/(1+dist(x_i, u_j)^2), then normalize it.\n",
    "        Arguments:\n",
    "            inputs: the variable containing data, shape=(n_samples, n_features)\n",
    "        Return:\n",
    "            q: student's t-distribution, or soft labels for each sample. shape=(n_samples, n_clusters)\n",
    "        \"\"\"\n",
    "        q = 1.0 / (1.0 + (K.sum(K.square(K.expand_dims(inputs, axis=1) - self.clusters), axis=2) / self.alpha))\n",
    "        q **= (self.alpha + 1.0) / 2.0\n",
    "        q = K.transpose(K.transpose(q) / K.sum(q, axis=1))\n",
    "        return q\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert input_shape and len(input_shape) == 2\n",
    "        return input_shape[0], self.n_clusters\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'n_clusters': self.n_clusters}\n",
    "        base_config = super(ClusteringLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "class DEC(object):\n",
    "    def __init__(self,\n",
    "                 dims,\n",
    "                 n_clusters=10,\n",
    "                 alpha=1.0,\n",
    "                 init='glorot_uniform'):\n",
    "\n",
    "        super(DEC, self).__init__()\n",
    "\n",
    "        self.dims = dims\n",
    "        self.input_dim = dims[0]\n",
    "        self.n_stacks = len(self.dims) - 1\n",
    "\n",
    "        self.n_clusters = n_clusters\n",
    "        self.alpha = alpha\n",
    "        self.autoencoder, self.encoder = autoencoder(self.dims, init=init)\n",
    "\n",
    "        # prepare DEC model\n",
    "        clustering_layer = ClusteringLayer(self.n_clusters, name='clustering')(self.encoder.output)\n",
    "        self.model = Model(inputs=self.encoder.input, outputs=clustering_layer)\n",
    "\n",
    "    def pretrain(self, x, y=None, optimizer='adam', epochs=200, batch_size=256, save_dir='results/temp'):\n",
    "        print('...Pretraining...')\n",
    "        self.autoencoder.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "        csv_logger = callbacks.CSVLogger(save_dir + '/pretrain_log.csv')\n",
    "        cb = [csv_logger]\n",
    "        if y is not None:\n",
    "            class PrintACC(callbacks.Callback):\n",
    "                def __init__(self, x, y):\n",
    "                    self.x = x\n",
    "                    self.y = y\n",
    "                    super(PrintACC, self).__init__()\n",
    "\n",
    "                def on_epoch_end(self, epoch, logs=None):\n",
    "                    if epoch % int(epochs/10) != 0:\n",
    "                        return\n",
    "                    feature_model = Model(self.model.input,\n",
    "                                          self.model.get_layer(\n",
    "                                              'encoder_%d' % (int(len(self.model.layers) / 2) - 1)).output)\n",
    "                    features = feature_model.predict(self.x)\n",
    "                    km = KMeans(n_clusters=len(np.unique(self.y)), n_init=20, n_jobs=4)\n",
    "                    y_pred = km.fit_predict(features)\n",
    "                    # print()\n",
    "                    print(' '*8 + '|==>  acc: %.4f,  nmi: %.4f  <==|'\n",
    "                          % (accuracy(self.y, y_pred), normalized_mutual_info_score(self.y, y_pred)))\n",
    "\n",
    "            cb.append(PrintACC(x, y))\n",
    "\n",
    "        # begin pretraining\n",
    "        t0 = time()\n",
    "        self.autoencoder.fit(x, x, batch_size=batch_size, epochs=epochs, callbacks=cb)\n",
    "        print('Pretraining time: ', time() - t0)\n",
    "        self.autoencoder.save_weights(save_dir + '/ae_weights.h5')\n",
    "        print('Pretrained weights are saved to %s/ae_weights.h5' % save_dir)\n",
    "        self.pretrained = True\n",
    "\n",
    "    def load_weights(self, weights):  # load weights of DEC model\n",
    "        self.model.load_weights(weights)\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        return self.encoder.predict(x)\n",
    "\n",
    "    def predict(self, x):  # predict cluster labels using the output of clustering layer\n",
    "        q = self.model.predict(x, verbose=0)\n",
    "        return q.argmax(1)\n",
    "\n",
    "    @staticmethod\n",
    "    def target_distribution(q):\n",
    "        weight = q ** 2 / q.sum(0)\n",
    "        return (weight.T / weight.sum(1)).T\n",
    "\n",
    "    def compile(self, optimizer='sgd', loss='kld'):\n",
    "        self.model.compile(optimizer=optimizer, loss=loss)\n",
    "\n",
    "    def fit(self, x, y=None, maxiter=2e4, batch_size=256, tol=1e-3,\n",
    "            update_interval=140, save_dir='./results/temp'):\n",
    "\n",
    "        print('Update interval', update_interval)\n",
    "        save_interval = x.shape[0] / batch_size * 5  # 5 epochs\n",
    "        print('Save interval', save_interval)\n",
    "\n",
    "        # Step 1: initialize cluster centers using k-means\n",
    "        t1 = time()\n",
    "        print('Initializing cluster centers with k-means.')\n",
    "        kmeans = KMeans(n_clusters=self.n_clusters, n_init=20)\n",
    "        y_pred = kmeans.fit_predict(self.encoder.predict(x))\n",
    "        y_pred_last = np.copy(y_pred)\n",
    "        self.model.get_layer(name='clustering').set_weights([kmeans.cluster_centers_])\n",
    "\n",
    "        # Step 2: deep clustering\n",
    "        # logging file\n",
    "        import csv\n",
    "        logfile = open(save_dir + '/dec_log.csv', 'w')\n",
    "        logwriter = csv.DictWriter(logfile, fieldnames=['iter', 'acc', 'nmi', 'ari', 'loss'])\n",
    "        logwriter.writeheader()\n",
    "\n",
    "        loss = 0\n",
    "        index = 0\n",
    "        index_array = np.arange(x.shape[0])\n",
    "        for ite in range(int(maxiter)):\n",
    "            if ite % update_interval == 0:\n",
    "                q = self.model.predict(x, verbose=0)\n",
    "                p = self.target_distribution(q)  # update the auxiliary target distribution p\n",
    "\n",
    "                # evaluate the clustering performance\n",
    "                y_pred = q.argmax(1)\n",
    "                if y is not None:\n",
    "                    acc = np.round(accuracy(y, y_pred), 5)\n",
    "                    nmi = np.round(normalized_mutual_info_score(y, y_pred), 5)\n",
    "                    ari = np.round(adjusted_rand_score(y, y_pred), 5)\n",
    "                    loss = np.round(loss, 5)\n",
    "                    logdict = dict(iter=ite, acc=acc, nmi=nmi, ari=ari, loss=loss)\n",
    "                    logwriter.writerow(logdict)\n",
    "                    print('Iter %d: acc = %.5f, nmi = %.5f, ari = %.5f' % (ite, acc, nmi, ari), ' ; loss=', loss)\n",
    "\n",
    "                # check stop criterion\n",
    "                delta_label = np.sum(y_pred != y_pred_last).astype(np.float32) / y_pred.shape[0]\n",
    "                y_pred_last = np.copy(y_pred)\n",
    "                if ite > 0 and delta_label < tol:\n",
    "                    print('delta_label ', delta_label, '< tol ', tol)\n",
    "                    print('Reached tolerance threshold. Stopping training.')\n",
    "                    logfile.close()\n",
    "                    break\n",
    "\n",
    "            # train on batch\n",
    "            # if index == 0:\n",
    "            #     np.random.shuffle(index_array)\n",
    "            idx = index_array[index * batch_size: min((index+1) * batch_size, x.shape[0])]\n",
    "            self.model.train_on_batch(x=x[idx], y=p[idx])\n",
    "            index = index + 1 if (index + 1) * batch_size <= x.shape[0] else 0\n",
    "\n",
    "            # save intermediate model\n",
    "            if ite % save_interval == 0:\n",
    "                print('saving model to:', save_dir + '/DEC_model_' + str(ite) + '.h5')\n",
    "                self.model.save_weights(save_dir + '/DEC_model_' + str(ite) + '.h5')\n",
    "\n",
    "            ite += 1\n",
    "\n",
    "        # save the trained model\n",
    "        logfile.close()\n",
    "        print('saving model to:', save_dir + '/DEC_model_final.h5')\n",
    "        self.model.save_weights(save_dir + '/DEC_model_final.h5')\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Pretraining...\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 15s 250us/step - loss: 0.0748\n",
      "        |==>  acc: 0.1892,  nmi: 0.1051  <==|\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 14s 237us/step - loss: 0.0636\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 14s 236us/step - loss: 0.0629\n",
      "        |==>  acc: 0.3910,  nmi: 0.2941  <==|\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 15s 252us/step - loss: 0.0541\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 16s 267us/step - loss: 0.0474\n",
      "        |==>  acc: 0.4658,  nmi: 0.4160  <==|\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 17s 290us/step - loss: 0.0401\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 17s 289us/step - loss: 0.0347\n",
      "        |==>  acc: 0.5547,  nmi: 0.5237  <==|\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 17s 289us/step - loss: 0.0311\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 18s 308us/step - loss: 0.0288\n",
      "        |==>  acc: 0.5653,  nmi: 0.5374  <==|\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 17s 276us/step - loss: 0.0268\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 17s 287us/step - loss: 0.0246\n",
      "        |==>  acc: 0.5538,  nmi: 0.5272  <==|\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 17s 284us/step - loss: 0.0231\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 17s 284us/step - loss: 0.0217\n",
      "        |==>  acc: 0.5903,  nmi: 0.5556  <==|\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 16s 274us/step - loss: 0.0207\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 16s 268us/step - loss: 0.0200\n",
      "        |==>  acc: 0.5977,  nmi: 0.5615  <==|\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 16s 267us/step - loss: 0.0193\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 16s 270us/step - loss: 0.0188\n",
      "        |==>  acc: 0.5784,  nmi: 0.5538  <==|\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 17s 275us/step - loss: 0.0183\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 17s 276us/step - loss: 0.0179\n",
      "        |==>  acc: 0.6901,  nmi: 0.6136  <==|\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 16s 272us/step - loss: 0.0177\n",
      "Pretraining time:  468.59100437164307\n",
      "Pretrained weights are saved to results/ae_weights.h5\n"
     ]
    }
   ],
   "source": [
    "# setting the hyper parameters\n",
    "init = 'glorot_uniform'\n",
    "pretrain_optimizer = 'adam'\n",
    "dataset = 'mnist'\n",
    "batch_size = 2048\n",
    "maxiter = 2e4\n",
    "tol = 0.001\n",
    "save_dir = 'results'\n",
    "\n",
    "import os\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "update_interval = 200\n",
    "pretrain_epochs = 20 #500\n",
    "init = VarianceScaling(scale=1. / 3., mode='fan_in',\n",
    "                       distribution='uniform')  # [-limit, limit], limit=sqrt(1./fan_in)\n",
    "#pretrain_optimizer = SGD(lr=1, momentum=0.9)\n",
    "\n",
    "\n",
    "# prepare the DEC model\n",
    "dec = DEC(dims=[X_train_reshaped.shape[-1], 500, 500, 2000, 10], n_clusters=10, init=init)\n",
    "\n",
    "dec.pretrain(x=X_train_reshaped, y=y_train_classes, optimizer=pretrain_optimizer,\n",
    "             epochs=pretrain_epochs, batch_size=batch_size,\n",
    "             save_dir=save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "encoder_0 (Dense)            (None, 500)               392500    \n",
      "_________________________________________________________________\n",
      "encoder_1 (Dense)            (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "encoder_2 (Dense)            (None, 2000)              1002000   \n",
      "_________________________________________________________________\n",
      "encoder_3 (Dense)            (None, 10)                20010     \n",
      "_________________________________________________________________\n",
      "clustering (ClusteringLayer) (None, 10)                100       \n",
      "=================================================================\n",
      "Total params: 1,665,110\n",
      "Trainable params: 1,665,110\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dec.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec.compile(optimizer=SGD(0.01, 0.9), loss='kld')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update interval 200\n",
      "Save interval 146.484375\n",
      "Initializing cluster centers with k-means.\n",
      "Iter 0: acc = 0.69502, nmi = 0.61950, ari = 0.53423  ; loss= 0\n",
      "saving model to: results/DEC_model_0.h5\n",
      "Iter 200: acc = 0.70308, nmi = 0.62660, ari = 0.54858  ; loss= 0\n",
      "Iter 400: acc = 0.70738, nmi = 0.63395, ari = 0.55879  ; loss= 0\n",
      "Iter 600: acc = 0.71025, nmi = 0.64341, ari = 0.56714  ; loss= 0\n",
      "Iter 800: acc = 0.71378, nmi = 0.65335, ari = 0.57532  ; loss= 0\n",
      "Iter 1000: acc = 0.71602, nmi = 0.66052, ari = 0.58028  ; loss= 0\n",
      "Iter 1200: acc = 0.71755, nmi = 0.66547, ari = 0.58373  ; loss= 0\n",
      "Iter 1400: acc = 0.71943, nmi = 0.66971, ari = 0.58691  ; loss= 0\n",
      "Iter 1600: acc = 0.72147, nmi = 0.67373, ari = 0.59025  ; loss= 0\n",
      "Iter 1800: acc = 0.72342, nmi = 0.67680, ari = 0.59269  ; loss= 0\n",
      "Iter 2000: acc = 0.72528, nmi = 0.67963, ari = 0.59492  ; loss= 0\n",
      "Iter 2200: acc = 0.72697, nmi = 0.68164, ari = 0.59712  ; loss= 0\n",
      "Iter 2400: acc = 0.72812, nmi = 0.68329, ari = 0.59847  ; loss= 0\n",
      "Iter 2600: acc = 0.72932, nmi = 0.68504, ari = 0.60011  ; loss= 0\n",
      "Iter 2800: acc = 0.72948, nmi = 0.68523, ari = 0.60035  ; loss= 0\n",
      "Iter 3000: acc = 0.73038, nmi = 0.68631, ari = 0.60146  ; loss= 0\n",
      "Iter 3200: acc = 0.73093, nmi = 0.68712, ari = 0.60226  ; loss= 0\n",
      "Iter 3400: acc = 0.73135, nmi = 0.68786, ari = 0.60303  ; loss= 0\n",
      "Iter 3600: acc = 0.73150, nmi = 0.68798, ari = 0.60311  ; loss= 0\n",
      "Iter 3800: acc = 0.73165, nmi = 0.68837, ari = 0.60353  ; loss= 0\n",
      "Iter 4000: acc = 0.73205, nmi = 0.68878, ari = 0.60417  ; loss= 0\n",
      "Iter 4200: acc = 0.73230, nmi = 0.68899, ari = 0.60441  ; loss= 0\n",
      "Iter 4400: acc = 0.73233, nmi = 0.68893, ari = 0.60445  ; loss= 0\n",
      "Iter 4600: acc = 0.73282, nmi = 0.68940, ari = 0.60511  ; loss= 0\n",
      "Iter 4800: acc = 0.73267, nmi = 0.68910, ari = 0.60484  ; loss= 0\n",
      "Iter 5000: acc = 0.73283, nmi = 0.68941, ari = 0.60505  ; loss= 0\n",
      "Iter 5200: acc = 0.73303, nmi = 0.68961, ari = 0.60539  ; loss= 0\n",
      "Iter 5400: acc = 0.73308, nmi = 0.68951, ari = 0.60534  ; loss= 0\n",
      "delta_label  0.0009666666666666667 < tol  0.001\n",
      "Reached tolerance threshold. Stopping training.\n",
      "saving model to: results/DEC_model_final.h5\n"
     ]
    }
   ],
   "source": [
    "y_pred = dec.fit(X_train_reshaped, y=y_train_classes, tol=tol, maxiter=maxiter, batch_size=batch_size,\n",
    "                 update_interval=update_interval, save_dir=save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_val = dec.predict(X_test_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6981151525389725"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_mutual_info_score(y_test_classes, pred_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
